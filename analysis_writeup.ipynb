{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas vs PostgreSQL\n",
    "\n",
    "Working on large data science projects usually involves the user accessing, manipulating, and retrieving data on a server. Next, the workflow moves client-side where the user will apply more refined data analysis and processing, typically tasks not possible or too clumsy to be done on the server. SQL (Structured Query Language) is ubiquitous in industry and data scientists will have to use it in their work to access data on the server.\n",
    "\n",
    "The line between what data manipulation should be done server-side using SQL or on the client-side using a language like Python is not clear. Further, people who are either uncomfortable or dislike using SQL may be tempted to keep server-side manipulation to a minimum and reserve more of those actions on the client-side. With powerful and popular Python libraries for data wrangling and manipulation, the temptation to keep server-side processing to a minimum has increased.\n",
    "\n",
    "This blog post will compare the time it takes to run typical data manipulation tasks like join, group by, etc. using PostgreSQL and pandas. PostgreSQL, often shortened as Postgres, is an object-relational database management system. It is free and open-source and runs on all major operating systems. Pandas is python data manipulation library that offers data structures akin to Excel spreadsheets and SQL tables and functions for manipulating those data structures.\n",
    "\n",
    "The performance will be measured for both tools for the following actions:\n",
    "\n",
    "- selecting columns\n",
    "- filtering rows\n",
    "- group by and aggregation\n",
    "- loading a large csv\n",
    "\n",
    "How these tasks scale as a function of table size will be expored by running the analysis with datasets with ten to ten million rows. These datasets are stored as csv files and have three column; the entries of the first two columns are floats while the third are strings. For each of the four tasks listed above, the benchmark will run one hundred replicates for each dataset size. The Postgre part of the benchmark was run with Python using psycopg2, a Postgre adapter for Python. Running Postgre directly showed no significant performance difference when using psycopg2, however, psycopg2 allows for easier scripting and better flexibility for running the benchmark. The computer used for this study runs Ubuntu 16.04, with 16 GB of RAM, and an 8 core process at 1.8 GHz. The code used for this benchmark can be found on GitHub.\n",
    "\n",
    "It is important for data scientists to know the limitations of their tools and what approaches are optimal in terms of time. Although smaller projects will not benefit a lot of speed up, small percentage gains in more data intensive applications will translate into large absolute time savings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Columns\n",
    "For this task, one column was selected from the datast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by and Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Large CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
